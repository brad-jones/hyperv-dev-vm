variables:
  # Use this just like you would a docker image tag.
  #
  # So if you do not override this value you can run this build as many times as
  # you like and there will only ever be a single image created, each time
  # overriding the previous.
  tag: latest

  # The EC2 instance type that will be used for the build,
  # irrelevant for other builders.
  instance_type: t3.micro

  # The ssh credentials to use to connect the provisioners
  ssh_username: packer
  ssh_private_key_file: ~/.ssh/id_rsa

builders:
  - type: hyperv-iso
    iso_url: https://download.fedoraproject.org/pub/fedora/linux/releases/31/Everything/x86_64/iso/Fedora-Everything-netinst-x86_64-31-1.9.iso
    iso_checksum: 559e82173d44d5500739d473a32e2bd24a66a49f0034020f9f86c9d5da6a2c61
    iso_checksum_type: sha256
    generation: 2
    disk_size: 200000
    disk_additional_size: 200000
    switch_name: Default Switch
    enable_secure_boot: false
    enable_dynamic_memory: true
    guest_additions_mode: none
    communicator: ssh
    ssh_username: "{{user `ssh_username`}}"
    ssh_private_key_file: "{{user `ssh_private_key_file`}}"
    ssh_timeout: 15m
    http_directory: .
    http_port_min: 8080
    http_port_max: 8080
    boot_wait: 5s
    boot_command:
      - c<wait>
      - linuxefi /images/pxeboot/vmlinuz ip=dhcp inst.ks=http://{{.HTTPIP}}:{{.HTTPPort}}/ks.cfg<enter>
      - initrdefi /images/pxeboot/initrd.img<enter>
      - boot<enter>
    shutdown_command: sudo shutdown now
    skip_export: true
    headless: false
    output_directory: dev-server-{{user `tag`}}

  - type: amazon-ebs
    instance_type: "{{user `instance_type`}}"
    ami_name: dev-server-{{user `ami_tag`}}
    ami_description: A Linux development environment
    source_ami_filter:
      most_recent: true
      owners: ["125523088429"]
      filters:
        architecture: x86_64
        virtualization-type: hvm
        root-device-type: ebs
        name: Fedora-Cloud-Base-31-2020*gp2*
    force_deregister: true
    force_delete_snapshot: true
    user_data_file: ./userdata.yml
    communicator: ssh
    ssh_username: "{{user `ssh_username`}}"
    ssh_private_key_file: "{{user `ssh_private_key_file`}}"

    # The "test" & "uat" accounts have a single VPC prefixed with the name
    # "core-network". The "prod" account has a VPC called "Application0".
    # This is just me inspecting the payroll accounts, not really sure if
    # this is a standard or not...
    vpc_filter:
      filters:
        "tag:Name": Application0

    # This filter might not be completely best practice,
    # ie: spinning up a build on a subnet called "SubnetAppELBTierB"
    # may not be appropriate but it does work.
    subnet_filter:
      most_free: true
      random: true
      filters:
        "tag:Name": Application0*

    # This build assumes it will be run inside one of Xero's "test" AWS accounts
    # which have direct network access from the office(s). Hence no need to
    # expose anything publicly.
    associate_public_ip_address: false

    # see: https://confluence.teamxero.com/display/SEC/AWS+Tagging+Standards
    #tags:
    #  Name: k8-node-lin-{{user `ami_tag`}}
    #  portfolio: pay
    #  product: crossproduct
    #  owner: peopleportfoliodevopsalerts@xero.com
    #  dataclassification: x5
    #  dataclassificationdate: 2020-02-20

provisioners:
  - type: shell
    script: provisioner.sh
